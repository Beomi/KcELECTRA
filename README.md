# KcELECTRA: Korean comments ELECTRA

** Updates on 2022.10.08 **

- KcELECTRA-base-v2022 (êµ¬ v2022-dev) ëª¨ë¸ ì´ë¦„ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
- ìœ„ ëª¨ë¸ì˜ ì„¸ë¶€ ìŠ¤ì½”ì–´ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.
- ê¸°ì¡´ KcELECTRA-base(v2021) ëŒ€ë¹„ ëŒ€ë¶€ë¶„ì˜ downstream taskì—ì„œ ~1%p ìˆ˜ì¤€ì˜ ì„±ëŠ¥ í–¥ìƒì´ ìˆìŠµë‹ˆë‹¤.

---

ê³µê°œëœ í•œêµ­ì–´ Transformer ê³„ì—´ ëª¨ë¸ë“¤ì€ ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ User-Generated Noisy text domain ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.

KcELECTRAëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ ELECTRAëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained ELECTRA ëª¨ë¸ì…ë‹ˆë‹¤.

ê¸°ì¡´ KcBERT ëŒ€ë¹„ ë°ì´í„°ì…‹ ì¦ê°€ ë° vocab í™•ì¥ì„ í†µí•´ ìƒë‹¹í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

KcELECTRAëŠ” Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)

```
ğŸ’¡ NOTE ğŸ’¡ 
General Corpusë¡œ í•™ìŠµí•œ KoELECTRAê°€ ë³´í¸ì ì¸ taskì—ì„œëŠ” ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
KcBERT/KcELECTRAëŠ” User genrated, Noisy textì— ëŒ€í•´ì„œ ë³´ë‹¤ ì˜ ë™ì‘í•˜ëŠ” PLMì…ë‹ˆë‹¤.
```

## KcELECTRA Performance

- Finetune ì½”ë“œëŠ” https://github.com/Beomi/KcBERT-finetune ì—ì„œ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•´ë‹¹ Repoì˜ ê° Checkpoint í´ë”ì—ì„œ Stepë³„ ì„¸ë¶€ ìŠ¤ì½”ì–´ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

|                    | Size<br/>(ìš©ëŸ‰) | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |
| :----------------- | :-------------: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |
| **KcELECTRA-base-v2022** |      475M       |     **91.97**      |         87.35          |       76.50        |        82.12         |           83.67           |          95.12          |         69.00 / 90.40         |
| **KcELECTRA-base** |      475M       |     91.71      |         86.90          |       74.80        |        81.65         |           82.65           |          **95.78**          |         70.60 / 90.11         |
| KcBERT-Base        |      417M       |       89.62        |         84.34          |       66.95        |        74.85         |           75.57           |            93.93            |         60.25 / 84.39         |
| KcBERT-Large       |      1.2G       |       90.68        |         85.53          |       70.15        |        76.99         |           77.49           |            94.06            |         62.16 / 86.64         |
| KoBERT             |      351M       |       89.63        |         86.11          |       80.65        |        79.00         |           79.64           |            93.93            |         52.81 / 80.27         |
| XLM-Roberta-Base   |      1.03G      |       89.49        |         86.26          |       82.95        |        79.92         |           79.09           |            93.53            |         64.70 / 88.94         |
| HanBERT            |      614M       |       90.16        |         87.31          |       82.40        |        80.89         |           83.33           |            94.19            |         78.74 / 92.02         |
| KoELECTRA-Base     |      423M       |       90.21        |         86.87          |       81.90        |        80.85         |           83.21           |            94.20            |         61.10 / 89.59         |
| KoELECTRA-Base-v2  |      423M       |       89.70        |         87.02          |       83.90        |        80.61         |           84.30           |            94.72            |         84.34 / 92.58         |
| KoELECTRA-Base-v3  |      423M       |       90.63        |       **88.11**        |     **84.45**      |      **82.24**       |         **85.53**         |            95.25            |       **84.83 / 93.45**       |
| DistilKoBERT       |      108M       |       88.41        |         84.13          |       62.55        |        70.55         |           73.21           |            92.48            |         54.12 / 77.80         |


\*HanBERTì˜ SizeëŠ” Bert Modelê³¼ Tokenizer DBë¥¼ í•©ì¹œ ê²ƒì…ë‹ˆë‹¤.

\***configì˜ ì„¸íŒ…ì„ ê·¸ëŒ€ë¡œ í•˜ì—¬ ëŒë¦° ê²°ê³¼ì´ë©°, hyperparameter tuningì„ ì¶”ê°€ì ìœ¼ë¡œ í•  ì‹œ ë” ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

## How to use

### Requirements

- `pytorch ~= 1.8.0`
- `transformers ~= 4.11.3`
- `emoji ~= 0.6.0`
- `soynlp ~= 0.0.493`

### Default usage

```python
from transformers import AutoTokenizer, AutoModel


tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base-v2022")
model = AutoModel.from_pretrained("beomi/KcELECTRA-base-v2022")

# êµ¬ë²„ì „(v2021)ì„ ì‚¬ìš©í•˜ê¸° ì›í•˜ì‹¤ ê²½ìš°
#tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
#model = AutoModel.from_pretrained("beomi/KcELECTRA-base")
```

> ğŸ’¡ ì´ì „ KcBERT ê´€ë ¨ ì½”ë“œë“¤ì—ì„œ `AutoTokenizer`, `AutoModel` ì„ ì‚¬ìš©í•œ ê²½ìš° `.from_pretrained("beomi/kcbert-base")` ë¶€ë¶„ì„ `.from_pretrained("beomi/KcELECTRA-base")` ë¡œë§Œ ë³€ê²½í•´ì£¼ì‹œë©´ ì¦‰ì‹œ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### Pretrain & Finetune Colab ë§í¬ ëª¨ìŒ 

#### Pretrain Data

- KcBERTí•™ìŠµì— ì‚¬ìš©í•œ ë°ì´í„° + ì´í›„ 2021.03ì›” ì´ˆê¹Œì§€ ìˆ˜ì§‘í•œ ëŒ“ê¸€
  - ì•½ 17GB
  - ëŒ“ê¸€-ëŒ€ëŒ“ê¸€ì„ ë¬¶ì€ ê¸°ë°˜ìœ¼ë¡œ Document êµ¬ì„±

#### Pretrain Code

- https://github.com/KLUE-benchmark/KLUE-ELECTRA Repoë¥¼ í†µí•œ Pretrain

#### Finetune Code

- https://github.com/Beomi/KcBERT-finetune Repoë¥¼ í†µí•œ Finetune ë° ìŠ¤ì½”ì–´ ë¹„êµ

#### Finetune Samples

- NSMC with PyTorch-Lightning 1.3.0, GPU, Colab <a href="https://colab.research.google.com/drive/1Hh63kIBAiBw3Hho--BvfdUWLu-ysMFF0?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>


## Train Data & Preprocessing

### Raw Data

í•™ìŠµ ë°ì´í„°ëŠ” 2019.01.01 ~ 2021.03.09 ì‚¬ì´ì— ì‘ì„±ëœ **ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤/í˜¹ì€ ì „ì²´ ë‰´ìŠ¤** ê¸°ì‚¬ë“¤ì˜ **ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€**ì„ ëª¨ë‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œì‹œ **ì•½ 17.3GBì´ë©°, 1ì–µ8ì²œë§Œê°œ ì´ìƒì˜ ë¬¸ì¥**ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.

> KcBERTëŠ” 2019.01-2020.06ì˜ í…ìŠ¤íŠ¸ë¡œ, ì •ì œ í›„ ì•½ 9ì²œë§Œê°œ ë¬¸ì¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

### Preprocessing

PLM í•™ìŠµì„ ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. í•œê¸€ ë° ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ê·¸ë¦¬ê³  ì´ëª¨ì§€(ğŸ¥³)ê¹Œì§€!

   ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ Emojiê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í–ˆìŠµë‹ˆë‹¤.

   í•œí¸, í•œê¸€ ë²”ìœ„ë¥¼ `ã„±-ã…ê°€-í£` ìœ¼ë¡œ ì§€ì •í•´ `ã„±-í£` ë‚´ì˜ í•œìë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤. 

2. ëŒ“ê¸€ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½

   `ã…‹ã…‹ã…‹ã…‹ã…‹`ì™€ ê°™ì´ ì¤‘ë³µëœ ê¸€ìë¥¼ `ã…‹ã…‹`ì™€ ê°™ì€ ê²ƒìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

3. Cased Model

   KcBERTëŠ” ì˜ë¬¸ì— ëŒ€í•´ì„œëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ëŠ” Cased modelì…ë‹ˆë‹¤.

4. ê¸€ì ë‹¨ìœ„ 10ê¸€ì ì´í•˜ ì œê±°

   10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.

5. ì¤‘ë³µ ì œê±°

   ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ëŒ“ê¸€ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ì¤‘ë³µ ëŒ“ê¸€ì„ í•˜ë‚˜ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

6. `OOO` ì œê±°

   ë„¤ì´ë²„ ëŒ“ê¸€ì˜ ê²½ìš°, ë¹„ì†ì–´ëŠ” ìì²´ í•„í„°ë§ì„ í†µí•´ `OOO` ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¡œ pipë¡œ ì„¤ì¹˜í•œ ë’¤, ì•„ë˜ cleaní•¨ìˆ˜ë¡œ í´ë¦¬ë‹ì„ í•˜ë©´ Downstream taskì—ì„œ ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. (`[UNK]` ê°ì†Œ)

```bash
pip install soynlp emoji
```

ì•„ë˜ `clean` í•¨ìˆ˜ë¥¼ Text dataì— ì‚¬ìš©í•´ì£¼ì„¸ìš”.

```python
import re
import emoji
from soynlp.normalizer import repeat_normalize

emojis = ''.join(emoji.UNICODE_EMOJI.keys())
pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-ã…£ê°€-í£{emojis}]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

import re
import emoji
from soynlp.normalizer import repeat_normalize

pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-ã…£ê°€-í£]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x): 
    x = pattern.sub(' ', x)
    x = emoji.replace_emoji(x, replace='') #emoji ì‚­ì œ
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x
```

> ğŸ’¡ Finetune Scoreì—ì„œëŠ” ìœ„ `clean` í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### Cleaned Data

- KcBERT ì™¸ ì¶”ê°€ ë°ì´í„°ëŠ” ì •ë¦¬ í›„ ê³µê°œ ì˜ˆì •ì…ë‹ˆë‹¤.


## Tokenizer, Model Train

TokenizerëŠ” Huggingfaceì˜ [Tokenizers](https://github.com/huggingface/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , ëª¨ë¸ì˜ General Downstream taskì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ KoELECTRAì—ì„œ ì‚¬ìš©í•œ Vocabì„ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì¶”ê°€ë¡œ ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. (ì‹¤ì œë¡œ ë‘ ëª¨ë¸ì´ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì€ ì•½ 5000í† í°ì´ì—ˆìŠµë‹ˆë‹¤.)

TPU `v3-8` ì„ ì´ìš©í•´ ì•½ 10ì¼ í•™ìŠµì„ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggingfaceì— ê³µê°œëœ ëª¨ë¸ì€ 848k stepì„ í•™ìŠµí•œ ëª¨ë¸ weightê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

(100k stepë³„ Checkpointë¥¼ í†µí•´ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ ë¶€ë¶„ì€ `KcBERT-finetune` repoë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.)

ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 100-200k ì‚¬ì´ì— ê¸‰ê²©íˆ Lossê°€ ì¤„ì–´ë“¤ë‹¤ í•™ìŠµ ì¢…ë£Œê¹Œì§€ë„ ì§€ì†ì ìœ¼ë¡œ lossê°€ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![KcELECTRA-base Pretrain Loss](https://cdn.jsdelivr.net/gh/beomi/blog-img@master/2021/04/07/image-20210407201231133.png)

### KcELECTRA Pretrain Stepë³„ Downstream task ì„±ëŠ¥ ë¹„êµ

> ğŸ’¡ ì•„ë˜ í‘œëŠ” ì „ì²´ ckptê°€ ì•„ë‹Œ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

![KcELECTRA Pretrain Stepë³„ Downstream task ì„±ëŠ¥ ë¹„êµ](https://cdn.jsdelivr.net/gh/beomi/blog-img@master/2021/04/07/image-20210407215557039.png)

- ìœ„ì™€ ê°™ì´ KcBERT-base, KcBERT-large ëŒ€ë¹„ **ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´** KcELECTRA-baseê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
- KcELECTRA pretrainì—ì„œë„ Train stepì´ ëŠ˜ì–´ê°ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì¸ìš©í‘œê¸°/Citation

KcELECTRAë¥¼ ì¸ìš©í•˜ì‹¤ ë•ŒëŠ” ì•„ë˜ ì–‘ì‹ì„ í†µí•´ ì¸ìš©í•´ì£¼ì„¸ìš”.

```
@misc{lee2021kcelectra,
  author = {Junbum Lee},
  title = {KcELECTRA: Korean comments ELECTRA},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Beomi/KcELECTRA}}
}
```

ë…¼ë¬¸ì„ í†µí•œ ì‚¬ìš© ì™¸ì—ëŠ” MIT ë¼ì´ì„¼ìŠ¤ë¥¼ í‘œê¸°í•´ì£¼ì„¸ìš”. â˜ºï¸

## Acknowledgement

KcELECTRA Modelì„ í•™ìŠµí•˜ëŠ” GCP/TPU í™˜ê²½ì€ [TFRC](https://www.tensorflow.org/tfrc?hl=ko) í”„ë¡œê·¸ë¨ì˜ ì§€ì›ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ ì¡°ì–¸ì„ ì£¼ì‹  [Monologg](https://github.com/monologg/) ë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤ :)

## Reference

### Github Repos

- [KcBERT by Beomi](https://github.com/Beomi/KcBERT)
- [BERT by Google](https://github.com/google-research/bert)
- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)
- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)
- [Transformers by Huggingface](https://github.com/huggingface/transformers)
- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)
- [ELECTRA train code by KLUE](https://github.com/KLUE-benchmark/KLUE-ELECTRA)

### Blogs

- [Monologgë‹˜ì˜ KoELECTRA í•™ìŠµê¸°](https://monologg.kr/categories/NLP/ELECTRA/)
- [Colabì—ì„œ TPUë¡œ BERT ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê¸° - Tensorflow/Google ver.](https://beomi.github.io/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)
